{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FinalProjectIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUBokUJoYOhp"
      },
      "source": [
        "# not a standatrd library\n",
        "!pip install livelossplot --quiet"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUcdEUdGSR5L"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from livelossplot import PlotLosses\n",
        "from utils import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn import svm\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p61W6ydN3Clr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTWfG516Sn0t"
      },
      "source": [
        "SEED = 123\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V8D9CWTS3lb"
      },
      "source": [
        "# load gpu usage and set dicts for data loading\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "!mkdir \"/content/images\"\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/images.zip\" \"/content/images\"\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/utils.py\" \".\"\n",
        "output = !unzip \"/content/images/images.zip\" -d \"/content/images\"\n",
        "wbc__label_name = {0: 'EOSINOPHIL', 1: 'LYMPHOCYTE', 2: 'MONOCYTE', 3: 'NEUTROPHIL'}\n",
        "wbc__name_label = {'EOSINOPHIL': 0, 'LYMPHOCYTE': 1, 'MONOCYTE': 2, 'NEUTROPHIL': 3}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJh2pL66zvK5"
      },
      "source": [
        "# function for loading the images from folders\n",
        "def load_from_folder(folder_path):\n",
        "  x = []\n",
        "  y = []\n",
        "  train_folder = folder_path\n",
        "  for wbc_type in os.listdir(train_folder):\n",
        "      if not wbc_type.startswith('.'):\n",
        "          label = wbc__name_label[wbc_type]\n",
        "          cell_type_folder = train_folder + wbc_type\n",
        "          for i, image_filename in enumerate(os.listdir(cell_type_folder)):\n",
        "              if not wbc_type.startswith('.'):\n",
        "                x.append(cell_type_folder + '/' + image_filename)\n",
        "                y.append(label)\n",
        "                # if i == 500:\n",
        "                #     break\n",
        "  return x, y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn2LizA10Clg"
      },
      "source": [
        "# loading the data\n",
        "train_folder = '/content/images/images/TRAIN/'\n",
        "test_folder = '/content/images/images/TEST/'\n",
        "images, labels = load_from_folder(train_folder)\n",
        "test_images, test_labels = load_from_folder(test_folder)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I63woAjZrYs"
      },
      "source": [
        "# transforming to tensors to calculate mean and std\n",
        "tensors_list = []\n",
        "tensor_test_list = []\n",
        "for idx, i in enumerate(images):\n",
        "  image = Image.open(i)\n",
        "  tensor_image = transforms.ToTensor()(image)\n",
        "  tensor_image = transforms.Resize((224, 224), interpolation=torchvision.transforms.InterpolationMode.NEAREST)(tensor_image)\n",
        "  tensors_list.append(tensor_image)\n",
        "\n",
        "for idx, i in enumerate(test_images):\n",
        "  image = Image.open(i)\n",
        "  tensor_image = transforms.ToTensor()(image)\n",
        "  tensor_image = transforms.Resize((224, 224), interpolation=torchvision.transforms.InterpolationMode.NEAREST)(tensor_image)\n",
        "  tensor_test_list.append(tensor_image)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJjfK1Ebf88S"
      },
      "source": [
        "# creating the trandform for my data class\n",
        "tensors = torch.stack(tensors_list, axis = 0)\n",
        "mean = tensors.mean().item()\n",
        "std = tensors.std().item()\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean = mean, std = std)])\n",
        "\n",
        "tensors = torch.stack(tensor_test_list, axis = 0)\n",
        "mean = tensors.mean().item()\n",
        "std = tensors.std().item()\n",
        "test_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean = mean, std = std)])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXUVTXkZaDAG"
      },
      "source": [
        "# My data class to handle the data with pytorch dataloader\n",
        "class Data(Dataset):\n",
        "\n",
        "    def __init__(self, x, y, transform):\n",
        "        self.x = x\n",
        "        self.labels = y\n",
        "        self.transform = transform\n",
        "        self.classes = {\"EOSINOPHIL\":1, \"LYMPHOCYTE\":2, \"MONOCYTE\":3, \"NEUTROPHIL\":4}\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.x[index]\n",
        "        img = Image.open(img_path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne6vq2c8LJbI"
      },
      "source": [
        "# First net - AlexNet\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes: int = 4) -> None:\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jag65_tmNFO"
      },
      "source": [
        "# second net - VGG 11\n",
        "configures = {\n",
        "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
        "    \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, global_params=None):\n",
        "        \n",
        "\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.features = make_layers(configures[global_params.configure], global_params.batch_norm)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(global_params.dropout_rate),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(global_params.dropout_rate),\n",
        "            nn.Linear(4096, global_params.num_classes),\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "        x = self.features(inputs)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, model_name, override_params=None):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        global_params = get_model_params(model_name, override_params)\n",
        "        return cls(global_params)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, num_classes=1000):\n",
        "        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n",
        "        load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000))\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def get_image_size(cls, model_name):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        _, res, _ = vgg_params(model_name)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _check_model_name_is_valid(cls, model_name):\n",
        "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
        "        the first four models (vgg{i} for i in 11,13,16,19) at the moment. \"\"\"\n",
        "        valid_models = ['vgg' + str(i) for i in [\"11\", \"11_bn\",\n",
        "                                                 \"13\", \"13_bn\",\n",
        "                                                 \"16\", \"16_bn\",\n",
        "                                                 \"19\", \"19_bn\"]]\n",
        "        if model_name not in valid_models:\n",
        "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))\n",
        "\n",
        "\n",
        "def make_layers(configure, batch_norm):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in configure:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXKO9TcnbhsR"
      },
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    prediction = y_pred.argmax(1, keepdim = True)\n",
        "    correct = prediction.eq(y.view_as(prediction)).sum()\n",
        "    accuracy = correct.float() / y.shape[0]\n",
        "    return accuracy"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EAyhU7dbnd1"
      },
      "source": [
        "# train function for the model\n",
        "def train(model, dataloader, optimizer, loss_function, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    \n",
        "    for x, y in dataloader:       \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)        \n",
        "        optimizer.zero_grad()          \n",
        "        y_pred = model(x)\n",
        "        if isinstance(y_pred, tuple):\n",
        "          y_pred = y_pred[0] \n",
        "        loss = loss_function(y_pred, y)     \n",
        "        acc = calculate_accuracy(y_pred, y)      \n",
        "        loss.backward()      \n",
        "        optimizer.step()       \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQHEXTYwbnhq"
      },
      "source": [
        "# validation function for the model\n",
        "def validate(model, dataloader, loss_function, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_pred = model(x)\n",
        "            if isinstance(y_pred, tuple):\n",
        "              y_pred = y_pred[0] \n",
        "            loss = loss_function(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dT5qxZvb1Xc"
      },
      "source": [
        "# predict function for the model\n",
        "def predict(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    images = []\n",
        "    labels = []\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y_pred = model(x)\n",
        "            y_prob = F.softmax(y_pred, dim = -1)\n",
        "            top_pred = y_prob.argmax(1, keepdim = True)\n",
        "            images.append(x.cpu())\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(y_prob.cpu())\n",
        "    images = torch.cat(images, dim = 0)\n",
        "    labels = torch.cat(labels, dim = 0)\n",
        "    probs = torch.cat(probs, dim = 0)\n",
        "\n",
        "    return images, labels, probs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zWn4zPv1IE5"
      },
      "source": [
        "# splitting the data with 0.8 train, 0.2 validation ratio\n",
        "c = list(zip(images, labels))\n",
        "random.shuffle(c)\n",
        "train_x_shuffle, train_y_shuffle = zip(*c)\n",
        "\n",
        "split_dataset = []\n",
        "split_labels = []\n",
        "\n",
        "split = 0.2\n",
        "last_idx = 0\n",
        "size = len(images)\n",
        "idx_train_img = idx_train_img = int(size * split)\n",
        "for i in range(5):\n",
        "  split_dataset.append(train_x_shuffle[last_idx:idx_train_img])\n",
        "  split_labels.append(train_y_shuffle[last_idx:idx_train_img])\n",
        "  last_idx = idx_train_img\n",
        "  split += 0.2\n",
        "  idx_train_img = idx_train_img = int(size * split)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOCxZNmpah9p"
      },
      "source": [
        "# K-Fold CV\n",
        "def kfold(fold, dataset, labels, bs=32):\n",
        "  valid = dataset[fold]\n",
        "  valid_labels = labels[fold]\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(5):\n",
        "    if i != fold:\n",
        "      for idx, j in enumerate(dataset[i]):\n",
        "        x.append(j)\n",
        "        y.append(labels[i][idx])\n",
        "  train_data = Data(x, y, transform)\n",
        "  validation_data = Data(valid, valid_labels, transform)\n",
        "  train_dataloader = DataLoader(train_data, shuffle=True, batch_size = bs)\n",
        "  valid_dataloader = DataLoader(validation_data, batch_size = bs)\n",
        "  return train_dataloader, valid_dataloader\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCzJVjrbrA_"
      },
      "source": [
        "# train and plotting portion\n",
        "avg_train_acc = 0\n",
        "avg_valid_acc = 0\n",
        "k_fold = 5\n",
        "for idx in range(k_fold):\n",
        "  \n",
        "  # setting model parameters\n",
        "  # model_to_use = AlexNet(num_classes=4)\n",
        "  model_to_use = VGG.from_name(\"vgg11\")\n",
        "  LR = 1e-5\n",
        "  optimizer = optim.Adam(model_to_use.parameters(), lr = LR)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  model_to_use = model_to_use.to(device)\n",
        "  loss_function = loss_function.to(device)\n",
        "\n",
        "  train_dataloader, valid_dataloader = kfold(idx, split_dataset, split_labels)\n",
        "  liveloss = PlotLosses()\n",
        "  results = {}\n",
        "  epochs = 8\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(epochs)):\n",
        "      \n",
        "      train_loss, train_acc = train(model_to_use, train_dataloader, optimizer, loss_function, device)\n",
        "      valid_loss, valid_acc = validate(model_to_use, valid_dataloader, loss_function, device)\n",
        "\n",
        "      results['train accuracy'] = train_acc\n",
        "      results['train loss'] = train_loss\n",
        "      results['validation accuracy'] = valid_acc   \n",
        "      results['validation loss'] = valid_loss\n",
        "      liveloss.update(results)\n",
        "      liveloss.send()\n",
        "\n",
        "      \n",
        "      print(f'Epoch: {epoch}')\n",
        "      print(f'Train Loss: {train_loss:.3f}  Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}   Validation Acc: {valid_acc*100:.2f}%')\n",
        "  avg_train_acc += train_acc\n",
        "  avg_valid_acc += valid_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt7VZ9Xv3G_r"
      },
      "source": [
        "# prepare the data for pytorch dataloader\n",
        "test_data = Data(test_images, test_labels, test_transform)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq15GQS7KPzh"
      },
      "source": [
        "print(\"Kfold train accuracy: \")\n",
        "print((avg_train_acc / 5) * 100)\n",
        "\n",
        "print(\"Kfold validation accuracy: \")\n",
        "print((avg_valid_acc / 5) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmqL8rLib1bj"
      },
      "source": [
        "# predictions\n",
        "images, labels, probs = predict(model_to_use, test_dataloader)\n",
        "pred_labels = torch.argmax(probs, 1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTFvzkXvayzj"
      },
      "source": [
        "accuracy = accuracy_score(labels, pred_labels)\n",
        "print(\"Accuracy of CNN model: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc62pikkcFZS"
      },
      "source": [
        "# confusion matrix\n",
        "classes = [\"EOSINOPHIL\", \"LYMPHOCYTE\", \"MONOCYTE\", \"NEUTROPHIL\"]\n",
        "fig = plt.figure(figsize = (10, 10));\n",
        "ax = fig.add_subplot(1, 1, 1);\n",
        "cm = confusion_matrix(labels, pred_labels);\n",
        "cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
        "cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
        "plt.xticks(rotation = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhkQC208a5Sn"
      },
      "source": [
        "# ROC AUC plot\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (10, 10))\n",
        "y_test = labels.detach().numpy()\n",
        "y_pred = pred_labels.detach().numpy()\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(y_test)\n",
        "y_test = lb.transform(y_test)\n",
        "y_pred = lb.transform(y_pred)\n",
        "for (idx, c_label) in enumerate(classes):\n",
        "    f, t, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
        "    c_ax.plot(f, t, label = '%s (AUC:%0.2f)'  % (c_label, auc(f, t)))\n",
        "c_ax.plot(f, f, 'b-', label = 'Random Guessing')\n",
        "c_ax.legend()\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')\n",
        "score = roc_auc_score(y_test, y_pred, average=\"macro\")\n",
        "print(f'ROC AUC score: {score}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTgygx9qPLtN"
      },
      "source": [
        "# SVM base model\n",
        "train = []\n",
        "train_labels = y\n",
        "validation_images = []\n",
        "validation_labels = valid_labels\n",
        "for img_path in x:\n",
        "  img = Image.open(img_path)\n",
        "  img = np.asarray(img).flatten()\n",
        "  train.append(img)\n",
        "for img_path in valid:\n",
        "  img = Image.open(img_path)\n",
        "  img = np.asarray(img).flatten()\n",
        "  validation_images.append(img)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWDRRl4c61JN"
      },
      "source": [
        "svm_model = OneVsRestClassifier(SVC(kernel='linear')).fit(train, train_labels)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiLPZsBY8BFR"
      },
      "source": [
        "predictions = svm_model.predict(validation_images)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z62jCrWs7iwE"
      },
      "source": [
        "accuracy = accuracy_score(validation_labels, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}